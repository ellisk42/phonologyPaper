\documentclass{article}

\usepackage{amsmath}
\usepackage[margin=0.1in]{geometry}
\usepackage{booktabs}
\usepackage{tipa}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{tikz}
\usetikzlibrary{fit}
%\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{amsfonts}       % blackboard math symbols

 % For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Automatically generating alternative hypotheses and controls}
%\author{Kevin Ellis, Timothy O'Donnell}

\newcommand{\nextForm}[1]{\rotatebox[origin=c]{270}{$_{\curvearrowright}$}$_{#1}$}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\Expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\indicator}{\mathds{1}} %{{\rm I\kern-.3em E}}
\newcommand{\expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}

\begin{document}
\maketitle


\section{Automatically generating grammars with Bayesian program learning}

\subsection{A Probabilistic Framing}

The goal of the grammar learner, whether it is a child, linguist, or computer program, is to
infer the grammar that gave rise to
a collection of utterances.
One way of
modeling this inference problem is to (1) place a prior distribution over grammars, for example, a distribution that puts more probability mass on shorter or simpler grammars; (2) equip each grammar with a
\emph{likelihood model} that specifies exactly how likely a grammar is to produce a given utterance;
and then (3) use Bayes's rule to work backwards from the
utterances to the grammar that was likely to produce them:
\begin{equation}
  \probability\left[\text{grammar}|\text{utterances} \right]\propto\underbrace{\probability\left[\text{utterances}|\text{grammar} \right]}_{\text{Likelihood}}\times\underbrace{\probability\left[\text{grammar} \right]}_{\text{Prior}}\label{posterior}
\end{equation}
We can think of the above equation as a recipe for
scoring how well a grammar explains a
collection of utterances.
Once we have a prior and a likelihood,
we know how to evaluate how well a grammar explains the data, modulo the assumptions implicit in the
prior and likelihood.

A simple and intuitive prior over grammars is one which penalizes longer grammars and favors parsimonious grammars, for example defining $\probability\left[\text{grammar} \right]\propto\exp\left(- \text{\# symbols in grammar} \right)$. An example of a likelihood model,
and the one we use here,
is just to count the number of extra bits or symbols needed to encode an utterance given the grammar\footnote{This is equivalent to a minimum description length (MDL) interpretation}.

Now in theory this probabilistic framing
has now bought us a procedure for automatically discovering
grammars that do a good job explaining the data.
In practice,
the space of all grammars is infinite and combinatorial,
and so cashing out this framing requires a
efficient procedure for searching the space of grammars and honing in on those that maximize Equation~\ref{posterior}.
A subfield of computer science, called \textbf{program synthesis},
has developed efficient techniques for
solving seemingly intractable combinatorial search problems
like those that arise in grammar learning.
We use a program synthesis tool called Sketch~\cite{solar2008program}
to search for grammars maximizing~\ref{posterior},
which was previously applied to language learning problems in~\cite{ellis2015unsupervised}.
For more detail on these program synthesis techniques we refer the reader to~\cite{solar2008program}.
This cashing out of the framing makes
our approach an instance of  \textbf{Bayesian Program Learning (BPL)}; see~\cite{lake2015human}.


\subsection{Modeling solutions to AGL problems}


The fundamental hypothesis underlying AGL research is that
artificial grammar learning must engage some shared resource with first language acquisition --
and so we can gain insights into first language acquisition by
instead studying AGL in controlled experiments.
Our contribution here is the demonstration that
a single BPL model can explain how a learner can quickly infer a grammar represented as SPE--style rules.
We collected grammars from AGL datasets~\cite{gerken2010infants,marcus1999rule,frank2011three}
and presented our BPL learner with data drawn from these grammars (Table~\ref{artificialGrammarTable}),
finding that our model automatically discovers the grammars that
 underly these data sets.
\begin{table}[h!]
\centering\begin{tabular}{lcll}
  \toprule
  Grammar&Example input to learner&MAP grammar&Natural language analogues\\\midrule
  \begin{tabular}{l}
    ABA (same/different/same)\\
  Marcus et al. 1999.     
    \end{tabular}&
  \begin{tabular}{l}
    wofewo\\
    lovilo\\
    fimufi
  \end{tabular}&
  \begin{tabular}{l}
$\varnothing\to \sigma_i$\verb| / #_|$\sigma \sigma_i$
  \end{tabular}& Reduplication (eg, Tagalog)
  \\\midrule
  \begin{tabular}{l}
      ABB (different/same/same)\\
  Marcus et al. 1999. 
    \end{tabular}&
  \begin{tabular}{l}
        wofefe\\
    lovivi\\
    fimumu
  \end{tabular}&
  \begin{tabular}{l}
$\varnothing\to \sigma_i$\verb| / |$\sigma_i$\verb|_#|
  \end{tabular}&
  Reduplication (eg, Tagalog)\\\midrule
  \begin{tabular}{l}
      ABx (different/different/constant)
    \end{tabular}&
  \begin{tabular}{l}
        wofeka\\
    lovika\\
    fimuka
  \end{tabular}&
  \begin{tabular}{l}
stem$ + x$
  \end{tabular}&concatenative morphology
  \\\midrule
  \begin{tabular}{l}
    AAx (same/same/constant)\\Gerken 2006. %~\cite{gerken2010infants}
    \end{tabular}&
  \begin{tabular}{l}
        wowoka\\
    loloka\\
    fifika
  \end{tabular}&
  \begin{tabular}{l}
    stem + $x$\\
$\varnothing\to \sigma_i$\verb| / #_|$\sigma_i$
  \end{tabular}&
  \begin{tabular}{l}
    reduplication\\concatenative morphology
    \end{tabular}\\\midrule
  \begin{tabular}{l}
    AxA (same/constant/same)\\Gerken 2006. %~\cite{gerken2010infants}
    \end{tabular}&
  \begin{tabular}{l}
        wokawo\\
    lokalo\\
    fikafi
  \end{tabular}&
  \begin{tabular}{l}
$x + $ stem\\
    $\varnothing$ $\to\sigma_i$  / \verb|# _| $\sigma$ $\sigma_i$
  \end{tabular}&
  \begin{tabular}{l}
    Infixing\\Reduplication
    \end{tabular}
  \\\midrule
  \begin{tabular}{l}
    Pig Latin
    \end{tabular}&
  \begin{tabular}{l}
    \textipa{pIg}$\to$\textipa{Igpe}\\
    \textipa{latIn}$\to$\textipa{atIle}\\
    \textipa{\ae sk}$\to$\textipa{\ae ske}\\
  \end{tabular}&
  \begin{tabular}{l}
      $\varnothing\to$\verb|C|$_i$ $/$ \verb|#C|$_i$\verb| [ ]* _ #|\\%
      $\varnothing\to$ \textipa{e} $/$ \verb| _ #|\\%
 \verb|C|$\to\varnothing$ $/$ \verb|# _|
  \end{tabular}  & \begin{tabular}{l}
    \textbf{Tim, is there an  analog?}\\
    \textbf{word initial segments moves to  }\\
    \textbf{word final?}
    \end{tabular}
  \bottomrule  \end{tabular}
\caption{Using BPL to model artificial grammar learning. Learner given 5 examples.}\label{artificialGrammarTable}
\end{table}

\subsection{Automatically exploring the space of alternative hypotheses}

Just like in first language acquisition,
these AGL learning setups introduce a trade-off between
grammars that are likely under the prior (and are therefore small; ``parsimony'')  and grammars that
assign high likelihood to the data (and therefore closely ``fit the data'', having high likelihood).
For example, a learner could infer a grammar that just memorized to the data (perfect fit but poor parsimony)
or it could infer a grammar that can generate every
possible word (parsimonious but a poor fit).
Effective grammar learners must navigate this trade-off.

To analyze this trade-off, we calculated its shape in the form of a
\textbf{pareto front}~\cite{mattson2005pareto}.
In our setting, a pareto front is the set of all grammars that
are not worse than another grammar along
the two competing axes of parsimony and fit to data.
Intuitively, grammars on the pareto front
are ones which an ideal Bayesian or MDL learner prefers,
\emph{independent} of how the learner decides to
relatively weight the prior and likelihood.
Figure~\ref{quadrupleFrontier}
diagrams the Pareto fronts for two AGL experiments
as the number of
example words provided to the learner is varied.
What these Pareto fronts show is (1)
the set of grammars entertained by the learner,
and (2) how the learner weighs these grammars against each other
as measured by the prior (parsimony) and the likelihood (fit to the data).
We believe that the Pareto front is a useful way of
viewing the space of possible grammars,
and understanding the different trade-offs
between the grammars.


\begin{figure}[H]

\includegraphics[width = 0.5\textwidth]{abb1.png} \includegraphics[width = 0.5\textwidth]{abb3.png}
\includegraphics[width = 0.5\textwidth]{aax1.png} \includegraphics[width = 0.5\textwidth]{aax3.png}
  
  \caption{Pareto fronts for ABB  (Marcus~1999; top two) \& AAX  (Gerken~2006; bottom two) learning problems fo either one example word (left column) or three example words  (right column). Rightward on x-axis corresponds to more parsimonious grammars and upward on y-axis corresponds to grammars that best fit the data, so the best grammars live in the upper right corners of the graphs. {\color{red}Red shade}: ground truth grammar. {\color{red!60}Pink shade}: shares structure with ground truth grammar. White shade: incorrect generalizations. As the number of examples increases, the Pareto fronts develop a sharp kink around the ground truth grammar, which indicates a stronger preference for the correct grammar. With one example the kinks can still exists but are less pronounced.}\label{quadrupleFrontier}
\end{figure}

\subsection{Beyond artificial grammars: Applying BPL to natural language}

A successful model of artificial grammar learning should also explain
at least some aspects of first language acquisition.  As a first step
in this direction, we take the same model that we applied to these AGL
learning problems and use it to solve phonology textbook exercises.
Each of these textbook exercises is an inductive reasoning problem
where the learner must explain surface pronunciations in terms of a
latent generative model (a grammar).
Figure~\ref{naturalLanguageExample} shows our model solving a
representative textbook problem.  We believe this extension two
natural language is an important next step for models of artificial
grammar learning,
and helps deliver on the basic promises and presuppositions of AGL research.



\begin{figure}[H]\centering
  \begin{minipage}{7cm}\centering
    \textbf{Textbook Tibetan counting problem:}
    \begin{tabular}{ll}\toprule
Number & Surface form
\\ \midrule
10 & \textipa{d\super Zu}\\
1 & \textipa{d\super Zig}\\
11 & \textipa{d\super Zugd\super Zig}\\
4 & \textipa{Si}\\
%14 & \textipa{d\super ZubSi}\\
40 & \textipa{Sibd\super Zu}\\
9 & \textipa{gu}\\
19 & \textipa{d\super Zurgu}\\
%90 & \textipa{gubd\super Zu}\\
5 & \textipa{Na}\\
%15 & \textipa{d\super ZuNa}\\
%50 & \textipa{Nabd\super Zu}\\
\bottomrule
\end{tabular}
  \end{minipage}%
  \hspace{0.5cm}%
  \begin{minipage}{6cm}\centering
    \textbf{Our model's solution:}
    
    Induced phonological rule: \texttt{C}$\to\varnothing /$\texttt{\#\_C} \\
    \hspace{0.5cm}\emph{(Reduce word initial consonant clusters)}

    Induced underlying representations:
\\\begin{tabular}{ll}\toprule
1 & \textipa{gd\super Zig}\\
4 & \textipa{bSi}\\
5 & \textipa{Na}\\
9 & \textipa{rgu}\\
10 & \textipa{bd\super Zu}
\bottomrule
\end{tabular}    
  \end{minipage}
  \caption{Example textbook phonology exercise (left, taken from~\cite{9780511808869}) and the solution our model discovers for it (right). To solve this problem the model must jointly infer unobserved underlying representations for each of the Tibetan numbers, along with a phonological rule that explains their surface pronunciations. The same code that calculated the Pareto fronts (Figure~\ref{quadrupleFrontier}) produces this analysis of the Tibetan count system.}\label{naturalLanguageExample}
  \end{figure}






%\pagebreak
%% \begin{figure}
%%   \begin{tikzpicture}
%%     %  \draw[help lines] (0,0) grid (10,-10);
%%     \node(UG)[purple,align = center,rotate = 90] at (1,-1) {\textbf{Universal}\\\textbf{Grammar}};
%%     %\draw[ultra thick] (2,0) rectangle (13,-2.3);


%%     \node(sounds) at (7,-0.3) {Phoneme Inventory};
%%     \node(soundExamples)[draw,align = center] at (7,-1.3) {\textipa{@, p, E, 2, k}\\\textipa{a, 3, A, O, d}\\$\cdots$};

%%     \node at (11,-0.3) {Program space};
%%     \node(programSpace)[draw,align = left] at (11,-1.3) {$\mathcal{M}\to \mathcal{A} | $\verb|stem|$|\mathcal{M}+\mathcal{M}$\\$\mathcal{P}\to \mathcal{R}\circ \mathcal{R}\circ \cdots \circ \mathcal{R}$};

%%     \node(featureSpace) at (3.5,-0.3) {Feature space};
%%     \node(featureExamples)[draw,align = center] at (3.5,-1.3) {\verb|voiced|, \verb|vowel|\\\verb|obstruent|, \verb|nasal|\\$\cdots$};

%%     \draw[thick,->] (featureExamples.east) -- (soundExamples.west);
%%     \draw[thick,->] (soundExamples.east) -- (programSpace.west);


%%     \node(universalBox)[purple,draw,ultra thick, fit = (sounds) (featureSpace) (featureExamples) (soundExamples) (programSpace)] {};
%%     \draw(lowerCausalArrow)[thick,->] (featureExamples.south) .. controls (7,-2.5) ..  (programSpace.south);

%%     \node(morphology)[blue,align = above,rotate = 90] at (1,-5) {\textbf{Programs}};
%%     \draw[ultra thick,->] (UG.west) -- (morphology.east);

%%     \node(ABAmorphologyLabel) at (10.5,-3.5) {Pig Latin Morphology};
%%     \node(ABAmorphology) [draw,align = left] at (10.5,-4){ \verb|observation|$\to$\verb|stem|};

%%     \node(ABAphonologyLabel) at (10.5,-5.3) {Pig Latin Phonology};
%%     \node(ABAphonology) [draw,align = left] at (10.5,-6.3) {%
%% $r_1$:      $\varnothing\to$\verb|C|$_i$ $/$ \verb|#C|$_i$\verb| [ ]* _ #|\\%
%%       $r_2$:      $\varnothing\to$ \textipa{e} $/$ \verb| _ #|\\%
%%     $r_3$: \verb|C|$\to\varnothing$ $/$ \verb|# _|};
%%     \node(ABAmorphologyBox)[blue,draw,ultra thick, fit = (ABAmorphologyLabel) (ABAphonology)] {};
    
%%     %  \draw[ultra thick] (2,-3) rectangle (7,-7);

%%     \node(polishMorphologyLabel) at (4.5,-3.5) {Polish Morphology};
%%     \node(polishMorphology) [draw,align = left] at (4.5,-4.3) {\verb|singular|$\to$ \verb|stem|\\\verb|plural|$\to$ \verb|stem|$ + $ /\textipa{i}/};

%%     \node at (4.5,-5.5) {Polish Phonology};
%%     \node(polishPhonology) [draw,align = left] at (4.5,-6.3) {%
%%       $r_1$: \textipa{o}$\to$\textipa{u}$/$\verb|_[-nasal +voice]#|\\%
%%       $r_2$: \verb|[-sonorant]|$\to$\verb|[-voice]|$/$\verb|_#|};
%%     \node(polishMorphologyBox)[blue,draw,ultra thick, fit = (polishMorphologyLabel) (polishPhonology)] {};

%%     \draw[ultra thick,->] (universalBox.south) -- (ABAmorphologyBox.north);
%%     \draw[ultra thick,->] (universalBox.south) -- (polishMorphologyBox.north);
    
%%     \node(lexicon)[magenta,align = center,rotate = 90] at (1,-10) {\textbf{Words}};
%%     \draw[ultra thick,->] (morphology.west)-- (lexicon.east);

%%     \node(p1) [align = left] at (9.5,-8) {\verb|LATIN:| \textipa{l\ae tIn}};
%%     \node(p2) [draw,align = left] at (9.5,-9.5) {\textipa{l\ae tIn}\nextForm{r_1}\\\textipa{l\ae tInl}\nextForm{r_2}\\\textipa{l\ae tInle}\nextForm{r_3}\\\textipa{\ae tInle}}; %\\\textipa{l\ae tInle}}};
%%     \draw [thick,->] (p1.south) -- (p2.north);
    
%%     \node(w1) [align = left] at (12,-8) {\verb|ASK:| \textipa{\ae sk}};
%%     \node(w2) [draw,align = left] at (12,-9.5) {\textipa{\ae sk}\nextForm{r_1}\\\textipa{\ae sk}\nextForm{r_2}\\\textipa{\ae ske}\nextForm{r_3}\\\textipa{\ae ske}};
%%     \draw [thick,->] (w1.south) -- (w2.north);
    
%%     \node[magenta,draw,align= left](ABAvocabulary) at (10.5,-11.5){\textipa{\ae tInle}, \textipa{\ae ske}, $\cdots$ };
%%     \draw [thick,->] (p2.south) -- (ABAvocabulary.north);
%%     \draw [thick,->] (w2.south) -- (ABAvocabulary.north);
%%     \node(ABAlexiconBox)[draw,ultra thick, fit = (p1) (w1) (ABAvocabulary)] {};
%%     \draw [ultra thick,->] (ABAmorphologyBox.south) -- (ABAlexiconBox.north);  

%%     \node(crib) at (4.5,-8) {\verb|horn:| \textipa{rog}};
%%     \node(Cingular)[draw,align = left] at (3.5,-9.5) {\verb|singular:|\\ \textipa{rog}\nextForm{r_1}\\\textipa{rug}\nextForm{r_2}\\\textipa{ruk}};
%%     \node(plural)[draw,align = left] at (5.5,-9.5) {\verb|plural:|\\ \textipa{rogi}\nextForm{r_1}\\\textipa{rogi}\nextForm{r_2}\\\textipa{rogi}};
%%     \draw [thick,->] (crib.south) -- (Cingular.north);
%%     \draw [thick,->] (crib.south) -- (plural.north);

%%     \node[magenta,draw,align = left] (polishObservation) at (4.5,-11.5) {\verb|horn, singular:| \textipa{\v{z}wop}\\\verb|horn, plural:| \textipa{\v{z}wubi}};
%%     \draw [thick,->] (Cingular.south) -- (polishObservation.north);
%%     \draw [thick,->] (plural.south) -- (polishObservation.north);

%%     \node(polishLexicon)[draw,ultra thick, fit = (crib) (Cingular) (plural) (polishObservation)] {};
%%     \draw [ultra thick,->] (polishMorphologyBox.south) -- (polishLexicon.north);
    
%%   \end{tikzpicture}
%% \end{figure}


%\bibliographystyle{named}
\bibliographystyle{unsrt} \bibliography{main}


\end{document}
